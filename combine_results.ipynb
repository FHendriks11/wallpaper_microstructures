{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each geometry:\n",
    "* combine output files from geometry generation and simulation into one pickle file with consistent conventions (e.g., all 0-indexing). Remove cases with overlapping edges.\n",
    "* create a .png with a visualization of the microstructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & path definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import funcs_helpers as fh\n",
    "import matplotlib.pyplot as plt\n",
    "import helper_funcs as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = r\"E:\\OneDrive - TU Eindhoven\\Werk\\matlab\\Martins_code\\data\"\n",
    "\n",
    "geometries_path = r\"E:\\OneDrive - TU Eindhoven\\Werk\\python\\wallpaper_dataset\\data\\dataset1\\simulated\"\n",
    "\n",
    "final_path = r\"E:\\OneDrive - TU Eindhoven\\Werk\\python\\wallpaper_dataset\\data\\dataset1\\final_dataset\\pickle_files\"\n",
    "\n",
    "# create final path if it does not exist\n",
    "if not os.path.exists(final_path):\n",
    "    os.makedirs(final_path)\n",
    "\n",
    "geoms = [f for f in os.listdir(geometries_path)]\n",
    "print(f'{len(geoms)} geometries found')\n",
    "\n",
    "# reruns of errorFlag trajs with perturbation error fixed\n",
    "reruns_path = r\"E:\\OneDrive - TU Eindhoven\\Werk\\matlab\\Martins_code\\data_reruns\"\n",
    "\n",
    "# reruns of remaining errorFlag trajs with smaller minimum time step\n",
    "reruns2_path = r\"E:\\OneDrive - TU Eindhoven\\Werk\\matlab\\Martins_code\\data_reruns2\"\n",
    "\n",
    "# reruns of too short trajs with smaller default time step\n",
    "reruns3_path = r\"E:\\OneDrive - TU Eindhoven\\Werk\\matlab\\Martins_code\\data_reruns3\"\n",
    "\n",
    "reruns = [f for f in os.listdir(reruns_path) if f.endswith('.mat')]\n",
    "reruns2 = [f for f in os.listdir(reruns2_path) if f.endswith('.mat')]\n",
    "reruns3 = [f for f in os.listdir(reruns3_path) if f.endswith('.mat')]\n",
    "\n",
    "refD_path = r'E:\\OneDrive - TU Eindhoven\\Werk\\matlab\\Martins_code\\data_refD'\n",
    "\n",
    "plot_contact = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if set of simulated geometries exactly matches the set of generated geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files1 = [f for f in os.listdir(geometries_path)]\n",
    "print('Nr of geometries:', len(files1))\n",
    "files2 = [f[:-4] for f in os.listdir(results_path) if f.endswith(\".mat\") and not f.endswith(\"specialnodes.mat\")]\n",
    "print('Nr of results:', len(files2))\n",
    "files_specnodes = [f[:-4] for f in os.listdir(results_path) if f.endswith(\"specialnodes.mat\")]\n",
    "print('Nr of special nodes files:', len(files_specnodes))\n",
    "files_specnodes2 = [f[:-17] for f in os.listdir(results_path) if f.endswith(\"specialnodes.mat\")]\n",
    "\n",
    "# check that for each geometry there is one result file and one special nodes file\n",
    "assert np.all(np.sort(files1) == np.sort(files2))\n",
    "assert np.all(np.sort(files1) == np.sort(files_specnodes2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate over all geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "rerun_found = False\n",
    "for g, geom in enumerate(geoms):\n",
    "    print(f'======= {g}, {geom} =======')\n",
    "    # ====================== LOAD SIMULATION DATA ======================\n",
    "    data = {'simulations': {'trajectories': {}, 'time_steps': {}},\n",
    "            'geometry': {'fundamental_domain': {}, 'unit_cell': {}},\n",
    "            'mesh': {'fundamental_domain': {}, 'unit_cell': {}, 'RVE': {}}\n",
    "            }\n",
    "    matfile = os.path.join(results_path, geom + '.mat')\n",
    "    data_from_mat = sio.loadmat(matfile)\n",
    "    for key in data_from_mat['data_sim'].dtype.names:\n",
    "        data['simulations']['trajectories'][key] = data_from_mat['data_sim'][key][0, 0]\n",
    "    for key in data_from_mat['data_ts'].dtype.names:\n",
    "        data['simulations']['time_steps'][key] = data_from_mat['data_ts'][key][0, 0]\n",
    "\n",
    "    # change from Matlab 1-indexing to Python 0-indexing\n",
    "    data['simulations']['time_steps']['traj'] -= 1\n",
    "\n",
    "    # reshuffle P\n",
    "    data['simulations']['time_steps']['P'] = data['simulations']['time_steps']['P'].reshape(-1, 4, order='F')[:, [0, 2, 3, 1]].reshape(-1,2,2)\n",
    "    # reshuffle D\n",
    "    data['simulations']['time_steps']['D'] = data['simulations']['time_steps']['D'].reshape(-1, 4, 4, order='F')[:, [[0], [2], [3], [1]], [[0, 2, 3, 1]]].reshape(-1, 2, 2, 2, 2)\n",
    "\n",
    "    # rename keys\n",
    "    data['simulations']['time_steps']['trajectory'] = data['simulations']['time_steps'].pop('traj')\n",
    "    if 'bifurcMode' not in data['simulations']['time_steps']:\n",
    "        print('!! Warning !! No bifurcation mode data found')\n",
    "        errors.append([geom, 'No bifurcation mode data found', 12])\n",
    "    else:\n",
    "        data['simulations']['time_steps']['bifurcation_mode'] = data['simulations']['time_steps'].pop('bifurcMode')\n",
    "    data['simulations']['trajectories']['error_flag'] = data['simulations']['trajectories'].pop('errorFlag')\n",
    "    data['simulations']['time_steps']['is_bifurcation_point'] = data['simulations']['time_steps'].pop('bifurc')\n",
    "\n",
    "    if len(data['simulations']['time_steps']['F']) == 0:\n",
    "        print('!! Warning !! No simulations found')\n",
    "        errors.append([geom, 'No simulations found', len(data['simulations']['trajectories']['F_final'])])\n",
    "        continue\n",
    "\n",
    "    # reshape/remove trailing dimension\n",
    "    for key in ['J_final', 't_final', 'phi_final', 'computation_time', 'error_flag']:\n",
    "        data['simulations']['trajectories'][key] = data['simulations']['trajectories'][key][:, 0]\n",
    "    data['simulations']['time_steps']['trajectory'] = data['simulations']['time_steps']['trajectory'][:, 0]\n",
    "    data['simulations']['time_steps']['is_bifurcation_point'] = data['simulations']['time_steps']['is_bifurcation_point'][:, 0]\n",
    "\n",
    "    # reshape bifurcation mode\n",
    "    if 'bifurcation_mode' in data['simulations']['time_steps']:\n",
    "        data['simulations']['time_steps']['bifurcation_mode'] = data['simulations']['time_steps']['bifurcation_mode'][0] # remove singleton dimension\n",
    "        for i, mode in enumerate(data['simulations']['time_steps']['bifurcation_mode']):\n",
    "            if mode.shape == (0, 0):\n",
    "                data['simulations']['time_steps']['bifurcation_mode'][i] = None\n",
    "            else:\n",
    "                data['simulations']['time_steps']['bifurcation_mode'][i] = mode.reshape(-1, 2)\n",
    "\n",
    "    lengths = [len(data['simulations']['time_steps'][key]) for key in data['simulations']['time_steps']]\n",
    "    if len(set(lengths)) != 1:\n",
    "        print('!! Warning !! Not all time steps have the same length')\n",
    "        print(lengths)\n",
    "        errors.append([geom, 'Missing time steps', max(lengths) - min(lengths)])\n",
    "        continue\n",
    "    print('Nr of time steps:', len(data['simulations']['time_steps']['F']))\n",
    "    print('Nr of nodes:', data['simulations']['time_steps']['microfluctuation'].shape[1])\n",
    "\n",
    "    # ====================== LOAD REF D ======================\n",
    "    path_temp = os.path.join(refD_path, geom + '.mat')\n",
    "    data_D_ref = sio.loadmat(path_temp)\n",
    "    # shuffle D\n",
    "    D_ref = data_D_ref['Cmacro'][[[0], [2], [3], [1]], [[0, 2, 3, 1]]].reshape(2,2,2,2)\n",
    "    data['simulations']['D_ref'] = D_ref\n",
    "\n",
    "    # print('data[time_steps][trajectory]:', data['simulations']['time_steps']['trajectory'])\n",
    "\n",
    "    # ====================== LOAD RERUN DATA ======================\n",
    "    for r, asdf in enumerate([reruns, reruns2, reruns3]):\n",
    "        for rerun in asdf:\n",
    "            if rerun.startswith(geom):  # check if one or more of the reruns belong to this geometry\n",
    "                print('Rerun found!', geom, rerun)\n",
    "                rerun_found = True\n",
    "                ind = int(os.path.splitext(rerun)[0].split('_')[-1]) - 1  # get index of this trajectory\n",
    "\n",
    "                # if r == 0:  # only for the first set of reruns\n",
    "                    # check if rerun indeed originally ended in an error\n",
    "                if not data['simulations']['trajectories']['error_flag'][ind] and not r==2:\n",
    "                    print('Rerun did not originally end in an error')\n",
    "                    continue\n",
    "\n",
    "                data_temp = {'simulations': {\n",
    "                                'trajectories': {},\n",
    "                                'time_steps': {}\n",
    "                                }\n",
    "                             }  # temporary dictionary to store rerun data\n",
    "\n",
    "                # needed:\n",
    "                # data_temp['simulations']['trajectories']['computation_time']\n",
    "                # data_temp['simulations']['trajectories']['error_flag']\n",
    "                # data_temp['simulations']['time_steps']: everything\n",
    "\n",
    "                if r==0:\n",
    "                    matfile = os.path.join(reruns_path, rerun)\n",
    "                elif r==1:\n",
    "                    matfile = os.path.join(reruns2_path, rerun)\n",
    "                elif r==2:\n",
    "                    matfile = os.path.join(reruns3_path, rerun)\n",
    "                data_from_mat_rerun = sio.loadmat(matfile)\n",
    "\n",
    "                # check if rerun actually has the same F\n",
    "                if not np.allclose(data['simulations']['trajectories']['F_final'][ind], data_from_mat_rerun['data_sim']['F_final'][0, 0]):\n",
    "                    print('F original:', data['simulations']['trajectories']['F_final'][ind])\n",
    "                    print('F rerun:', data_from_mat_rerun['data_sim']['F_final'][0, 0])\n",
    "                    print('Rerun does not have the same F_final')\n",
    "                    continue\n",
    "\n",
    "                # fill the data_temp dictionary\n",
    "                for key in data_from_mat_rerun['data_sim'].dtype.names:\n",
    "                    data_temp['simulations']['trajectories'][key] = data_from_mat_rerun['data_sim'][key][0, 0]\n",
    "                for key in data_from_mat_rerun['data_ts'].dtype.names:\n",
    "                    data_temp['simulations']['time_steps'][key] = data_from_mat_rerun['data_ts'][key][0, 0]\n",
    "\n",
    "                # change from Matlab 1-indexing to Python 0-indexing\n",
    "                del data_temp['simulations']['time_steps']['traj']  # will always be 1 anyways\n",
    "\n",
    "\n",
    "                # reshuffle P\n",
    "                data_temp['simulations']['time_steps']['P'] = data_temp['simulations']['time_steps']['P'].reshape(-1, 4, order='F')[:, [0, 2, 3, 1]].reshape(-1,2,2)\n",
    "                # reshuffle D\n",
    "                data_temp['simulations']['time_steps']['D'] = data_temp['simulations']['time_steps']['D'].reshape(-1, 4, 4, order='F')[:, [[0], [2], [3], [1]], [[0, 2, 3, 1]]].reshape(-1, 2, 2, 2, 2)\n",
    "\n",
    "                # rename keys\n",
    "                if 'bifurcMode' not in data_temp['simulations']['time_steps']:\n",
    "                    print('!! Warning !! No bifurcation mode data_temp found')\n",
    "                    errors.append([geom, 'No bifurcation mode data_temp found', 12])\n",
    "                else:\n",
    "                    data_temp['simulations']['time_steps']['bifurcation_mode'] = data_temp['simulations']['time_steps'].pop('bifurcMode')\n",
    "                data_temp['simulations']['trajectories']['error_flag'] = data_temp['simulations']['trajectories'].pop('errorFlag')\n",
    "                data_temp['simulations']['time_steps']['is_bifurcation_point'] = data_temp['simulations']['time_steps'].pop('bifurc')\n",
    "\n",
    "                if len(data_temp['simulations']['time_steps']['F']) == 0:\n",
    "                    print('!! Warning !! Rerun has 0 time steps')\n",
    "                    errors.append([geom, 'Rerun has 0 time steps', 1, [ind]])\n",
    "                    continue\n",
    "\n",
    "                # reshape/remove trailing dimension\n",
    "                for key in ['computation_time', 'error_flag']:\n",
    "                    data_temp['simulations']['trajectories'][key] = data_temp['simulations']['trajectories'][key][:, 0]\n",
    "                data_temp['simulations']['time_steps']['is_bifurcation_point'] = data_temp['simulations']['time_steps']['is_bifurcation_point'][:, 0]\n",
    "\n",
    "                if len(data_temp['simulations']['time_steps']['F']) == 0:\n",
    "                    raise ValueError('Rerun has length 0')\n",
    "\n",
    "                data_temp['simulations']['time_steps']['trajectory'] = np.array([ind]*len(data_temp['simulations']['time_steps']['F']))  # set trajectory to the index of the rerun\n",
    "\n",
    "                # reshape bifurcation mode\n",
    "                if 'bifurcation_mode' in data_temp['simulations']['time_steps']:\n",
    "                    data_temp['simulations']['time_steps']['bifurcation_mode'] = data_temp['simulations']['time_steps']['bifurcation_mode'][0] # remove singleton dimension\n",
    "                    for i, mode in enumerate(data_temp['simulations']['time_steps']['bifurcation_mode']):\n",
    "                        if mode.shape == (0, 0):\n",
    "                            data_temp['simulations']['time_steps']['bifurcation_mode'][i] = None\n",
    "                        else:\n",
    "                            data_temp['simulations']['time_steps']['bifurcation_mode'][i] = mode.reshape(-1, 2)\n",
    "\n",
    "                lengths = [len(data_temp['simulations']['time_steps'][key]) for key in data_temp['simulations']['time_steps']]\n",
    "                if len(set(lengths)) != 1:\n",
    "                    print('!! Warning !! Not all time steps have the same length')\n",
    "                    print(lengths)\n",
    "                    errors.append([geom, 'Missing time steps', max(lengths) - min(lengths)])\n",
    "                    continue\n",
    "\n",
    "                # insert new data into the original data\n",
    "                inds_replace = np.where(data['simulations']['time_steps']['trajectory'] == ind)[0]\n",
    "                if len(inds_replace) > 0:\n",
    "                    start_ind = inds_replace[0]\n",
    "                    end_ind = inds_replace[-1] + 1\n",
    "                else:  # if the original trajectory had 0 time steps, you can't find the start and end indices\n",
    "                    # find last index where ind < trajectory\n",
    "                    inds_temp = np.where(data['simulations']['time_steps']['trajectory'] < ind)[0]\n",
    "                    if len(inds_temp) > 0:\n",
    "                        start_ind = inds_temp[-1] + 1\n",
    "                        end_ind = start_ind\n",
    "                    else:  # if there are no trajectories < ind, it must be the first trajectory\n",
    "                        start_ind = 0\n",
    "                        end_ind = 0\n",
    "\n",
    "                for key in data['simulations']['time_steps']:\n",
    "                    data['simulations']['time_steps'][key] = np.concatenate([data['simulations']['time_steps'][key][:start_ind], data_temp['simulations']['time_steps'][key], data['simulations']['time_steps'][key][end_ind:]])  # replace the data of the rerun\n",
    "\n",
    "                data['simulations']['trajectories']['error_flag'][ind] = data_temp['simulations']['trajectories']['error_flag'][0]  # replace the error flag\n",
    "                data['simulations']['trajectories']['computation_time'][ind] = data_temp['simulations']['trajectories']['computation_time'][0]  # replace the computation time\n",
    "\n",
    "    # Remove empty trajectories\n",
    "    tr = data['simulations']['time_steps']['trajectory']\n",
    "    n = len(data['simulations']['trajectories']['F_final'])\n",
    "    tr2, inv = np.unique(tr, return_inverse=True)\n",
    "    counts = np.bincount(tr, minlength=n)\n",
    "    if len(tr2) != n:\n",
    "        print('!! Warning !! Not all trajectories are present')\n",
    "        errors.append([geom, 'Trajectories of length 0 in data', n-len(tr2), np.where(counts == 0)[0]])\n",
    "        # remove trajectories of length 0\n",
    "        for key in data['simulations']['trajectories']:\n",
    "            data['simulations']['trajectories'][key] = data['simulations']['trajectories'][key][counts > 0]\n",
    "        data['simulations']['time_steps']['trajectory'] = inv\n",
    "\n",
    "    # ====================== LOAD SPECIAL NODES DATA ======================\n",
    "    matfile = os.path.join(results_path, geom + '_specialnodes.mat')\n",
    "    data_from_mat_specialnodes = sio.loadmat(matfile)\n",
    "    # change from Matlab 1-indexing to Python 0-indexing\n",
    "    data['mesh']['RVE']['fixed_node'] = data_from_mat_specialnodes['fixed_node'][0,0]-1\n",
    "    data['mesh']['RVE']['source_nodes'] = data_from_mat_specialnodes['source_nodes'][:, 0]-1\n",
    "    data['mesh']['RVE']['image_nodes'] = data_from_mat_specialnodes['image_nodes'][:, 0]-1\n",
    "\n",
    "    # ====================== LOAD GEOMETRY DATA (.mat) ======================\n",
    "    # load data that was sent to matlab\n",
    "    path = os.path.join(geometries_path, geom, geom + '_00.mat')\n",
    "    data_to_mat = sio.loadmat(path)\n",
    "\n",
    "    data['mesh']['RVE']['p'] = data_to_mat['p']\n",
    "    data['mesh']['RVE']['t'] = data_to_mat['t']\n",
    "    data['mesh']['RVE']['boundary_inds'] = data_to_mat['boundary_inds']\n",
    "    data['mesh']['RVE']['inds_per_fd'] = data_to_mat['inds_per_fd']\n",
    "    data['mesh']['RVE']['volume_fraction'] = data_to_mat['volume_fraction'][0,0]\n",
    "\n",
    "    # check dtype\n",
    "    if data['mesh']['RVE']['boundary_inds'].dtype == object:\n",
    "        b = data['mesh']['RVE']['boundary_inds'][0]\n",
    "        data['mesh']['RVE']['boundary_inds'] = [b2[0].tolist() for b2 in b]\n",
    "    else:\n",
    "        data['mesh']['RVE']['boundary_inds'] = data['mesh']['RVE']['boundary_inds'].tolist()\n",
    "    # print(len(data['mesh']['RVE']['boundary_inds']), len(data['mesh']['RVE']['boundary_inds'][0]), type(data['mesh']['RVE']['boundary_inds']), type(data['mesh']['RVE']['boundary_inds'][0]))\n",
    "\n",
    "    # ====================== LOAD GEOMETRY DATA (.pkl) ======================\n",
    "    # load data from .pkl file from geometry generation\n",
    "    with open(os.path.join(geometries_path, geom, geom + '_00.pkl'), 'rb') as f:\n",
    "        data_pkl = pickle.load(f)\n",
    "\n",
    "    # add unit cell data\n",
    "    for key, value in data_pkl['uc'].items():\n",
    "        data['geometry']['unit_cell'][key] = value\n",
    "\n",
    "    # rename lattice vectors to lattice_vectors\n",
    "    data['geometry']['unit_cell']['lattice_vectors'] = data['geometry']['unit_cell'].pop('lattice vectors')\n",
    "\n",
    "    # delete equiv_nodes, is equivalent to periodic_nodes\n",
    "    del data['geometry']['unit_cell']['equiv_nodes']\n",
    "\n",
    "    # add fundamental domain data\n",
    "    for key, value in data_pkl['fd'].items():\n",
    "        data['geometry']['fundamental_domain'][key] = value\n",
    "\n",
    "    # delete unnecessary keys\n",
    "    del data['geometry']['fundamental_domain']['points']\n",
    "    del data['geometry']['fundamental_domain']['bound_inds']\n",
    "    del data['geometry']['fundamental_domain']['edges']\n",
    "    del data['geometry']['fundamental_domain']['n_points']\n",
    "    del data['geometry']['fundamental_domain']['n_edges']\n",
    "\n",
    "    # add unique holes data\n",
    "    data['geometry']['unique_faces'] = data_pkl['unique_holes']\n",
    "\n",
    "    uf = data['geometry']['unique_faces']\n",
    "    for elem in uf:\n",
    "        if 'equiv_holes' in elem:\n",
    "            elem['equiv_faces'] = elem.pop('equiv_holes')\n",
    "        if 'all_holes' in elem:\n",
    "            elem['all_faces'] = elem.pop('all_holes')\n",
    "\n",
    "    for elem in data['geometry']['unique_faces']:\n",
    "        # to do: might throw an error if bisectors_x or splint_point_dists are not defined\n",
    "        if 'bisectors_x' in elem:\n",
    "            elem['bisectors_x'] = np.array(elem['bisectors_x'])\n",
    "        if 'spline_point_dists' in elem:\n",
    "            elem['spline_point_dists'] = np.array(elem['spline_point_dists'])\n",
    "\n",
    "    data['mesh']['fundamental_domain']['p'] = data_pkl['p']\n",
    "    data['mesh']['fundamental_domain']['t'] = data_pkl['t']\n",
    "\n",
    "    data['mesh']['unit_cell']['p'] = data_pkl['p_all']\n",
    "    data['mesh']['unit_cell']['t'] = data_pkl['t_all']\n",
    "    data['mesh']['unit_cell']['boundary_inds'] = data_pkl['boundary_inds']\n",
    "\n",
    "    # ====================== CREATE convenient quantities ======================\n",
    "    # Calculate position of points at each time step\n",
    "    F = data['simulations']['time_steps']['F']  # shape (n_time_steps, 2, 2)\n",
    "    x_0 = data['mesh']['RVE']['p']  # shape (n_points, 2)\n",
    "    w = data['simulations']['time_steps']['microfluctuation']  # shape (n_time_steps, n_points, 2)\n",
    "    x = np.einsum('ijk,lk->ilj', F, x_0) + w\n",
    "    data['simulations']['time_steps']['x'] = x\n",
    "\n",
    "    # Create edges\n",
    "    # turn the elements into edges and deduplicate them\n",
    "    t = data['mesh']['RVE']['t']\n",
    "    edges = np.vstack((t[:, [0, 3]],\n",
    "                    t[:, [3, 1]],\n",
    "                    t[:, [1, 4]],\n",
    "                    t[:, [4, 2]],\n",
    "                    t[:, [2, 5]],\n",
    "                    t[:, [5, 0]]))\n",
    "    edges2 = np.sort(edges, axis=1)\n",
    "    edges3, counts = np.unique(edges2, axis=0, return_counts=True)\n",
    "    data['mesh']['RVE']['edges'] = edges3\n",
    "\n",
    "    # find all edges that are not shared by two triangles: boundary edges\n",
    "    # (either boundary of the RVE of boundary of a hole)\n",
    "    # for each edge in the boundary, check if both nodes are in the same array in boundary_inds\n",
    "    # if so, it is a unit cell boundary edge\n",
    "    b_edges = edges3[counts!=2]\n",
    "    bools = np.zeros(len(b_edges), dtype=bool)\n",
    "    # print(len(data['mesh']['RVE']['boundary_inds']), len(data['mesh']['RVE']['boundary_inds'][0]))\n",
    "    for b in data['mesh']['RVE']['boundary_inds']:\n",
    "        bools[np.isin(b_edges[:, 0], b)*np.isin(b_edges[:, 1], b)] = True\n",
    "    bools2 = np.zeros(len(edges3), dtype=bool)\n",
    "    bools2[counts!=2] = bools\n",
    "    data['mesh']['RVE']['boundary_edges_inds'] = np.where(bools2)[0]\n",
    "\n",
    "    # select hole boundary edges\n",
    "    # (edges that are boundary edges but not unit cell boundary edges)\n",
    "    bools3 = np.zeros(len(edges3), dtype=bool)\n",
    "    bools3[counts!=2] = True  # all boundary edges\n",
    "    bools3[bools2] = False     # remove unit cell boundary edges, leaving only hole boundary edges  #!! edge case: if a hole boundary edge is also a unit cell boundary edge, this will go wrong!!\n",
    "    data['mesh']['RVE']['hole_boundary_edges_inds'] = np.where(bools3)[0]\n",
    "\n",
    "    # ====================== REMOVE CONTACT ======================\n",
    "    edges = data['mesh']['RVE']['edges']\n",
    "    hb_inds = data['mesh']['RVE']['hole_boundary_edges_inds']\n",
    "    hb_edges = edges[hb_inds]\n",
    "    # only use coordinates of hole boundary nodes and edges at the hole boundary\n",
    "    hb_nodes, hb_edges = np.unique(hb_edges, return_inverse=True)\n",
    "    hb_edges = hb_edges.reshape(-1, 2)\n",
    "    hb_x = data['simulations']['time_steps']['x'][:, hb_nodes]\n",
    "\n",
    "    # specify end and start of each trajectory\n",
    "    traj = data['simulations']['time_steps']['trajectory']\n",
    "    n_trajs = np.max(traj)+1\n",
    "    start_inds = np.concatenate(([0], np.where(traj[:-1] != traj[1:])[0]+1))\n",
    "    end_inds = np.concatenate((start_inds[1:], [len(traj)]))\n",
    "\n",
    "    ends_in_contact = np.zeros(n_trajs, dtype=bool)\n",
    "    i = 0\n",
    "    while i < len(traj):\n",
    "        # print(i)\n",
    "        traj_i = traj[i]\n",
    "\n",
    "        # Get the positions of the nodes\n",
    "        x = data['simulations']['time_steps']['x'][i]\n",
    "        hb_x_i = hb_x[i]\n",
    "\n",
    "        # check if a pair of edges intersects\n",
    "        cr = mf.crossing_edges(hb_x_i, hb_edges)\n",
    "\n",
    "        if False:\n",
    "            # plot all nodes\n",
    "            plt.figure()\n",
    "            plt.scatter(*(hb_x_i.T), s=2, c='tab:blue')\n",
    "\n",
    "            # plot boundary edges\n",
    "            x, y = hb_x_i.T[:, hb_edges]\n",
    "            x, y = x.T, y.T\n",
    "            plt.plot(x, y, c='tab:blue', alpha=1, zorder=-1)\n",
    "\n",
    "            plt.gca().set_aspect('equal')\n",
    "            plt.grid()\n",
    "\n",
    "        if cr.size > 0:\n",
    "            ends_in_contact[traj_i] = True\n",
    "            # print(f'i={i}, traj={traj[i]}, contact detected')\n",
    "\n",
    "            if False:\n",
    "                # plot all nodes\n",
    "                plt.figure()\n",
    "                plt.scatter(*(hb_x_i.T), s=2, c='tab:blue')\n",
    "\n",
    "                # plot boundary edges\n",
    "                x, y = hb_x_i.T[:, hb_edges]\n",
    "                x, y = x.T, y.T\n",
    "                plt.plot(x, y, c='tab:blue', alpha=1, zorder=-1)\n",
    "\n",
    "                # plot nodes of crossing edges\n",
    "                nodes = hb_edges[cr].reshape(-1, 4)\n",
    "                nodes2 = np.unique(nodes)\n",
    "                plt.scatter(*(hb_x_i[nodes2].T), s=2, c='tab:orange')\n",
    "\n",
    "                # plot crossing edges\n",
    "                x, y = hb_x_i.T[:, hb_edges[cr].reshape(-1, 2)]\n",
    "                x, y = x.T, y.T\n",
    "                plt.plot(x, y, c='tab:orange', alpha=1, zorder=-1)\n",
    "\n",
    "                plt.gca().set_aspect('equal')\n",
    "                plt.grid()\n",
    "                plt.savefig(os.path.join(final_path, geom + f'_contact_traj{traj_i}.png'))\n",
    "                plt.close()\n",
    "\n",
    "            end_inds[traj_i] = i # this trajectory ends early\n",
    "            if traj_i == n_trajs-1: # last trajectory, break for loop\n",
    "                break\n",
    "            else: # skip rest of the time steps of this trajectory, go to start of next trajectory\n",
    "                if i > start_inds[traj_i+1]:\n",
    "                    raise ValueError('i should go back to a previous time step, which makes no sense')\n",
    "                i = start_inds[traj_i+1]\n",
    "\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "\n",
    "    inds_to_keep = np.concatenate([np.arange(i,j) for i,j in zip(start_inds, end_inds)])\n",
    "    # print(f'start_inds: \\t{start_inds}')\n",
    "    # print(f'end_inds: \\t{end_inds}')\n",
    "    print(f'Trajectory lengths: {end_inds-start_inds}')\n",
    "    # print(f'ends_in_contact: \\t{ends_in_contact.astype(int)}')\n",
    "    # print(f'ErrorFlag: \\t{data[\"simulations\"][\"error_flag\"]}')\n",
    "    error2 = data['simulations']['trajectories']['error_flag']\n",
    "    error2[ends_in_contact] = False\n",
    "    print(f'Error before contact: {error2}')\n",
    "\n",
    "    if np.any(end_inds - start_inds <= 2):\n",
    "        print('!! Warning !! One or more trajectories have length <= 2')\n",
    "        errors.append([geom, 'Trajectories of length <= 2 after removing contact', np.sum(end_inds-start_inds <= 2), np.where(end_inds-start_inds <= 2)[0]])\n",
    "    if np.any(error2):\n",
    "\n",
    "        print('!! Warning !! One or more trajectories still end in an error when contact is removed')\n",
    "        n_err = error2.sum()\n",
    "        errors.append([geom, 'Error remains after removing contact', n_err, np.where(error2)[0]])\n",
    "\n",
    "        # if True:\n",
    "        #     # plot all nodes\n",
    "        #     plt.figure()\n",
    "        #     plt.suptitle(geom + ' - trajectory ending in an error')\n",
    "        #     # of the first trajectory ending in an error, take last time step\n",
    "        #     ind_temp = end_inds[error2][0] - 1\n",
    "        #     hb_x_i = hb_x[ind_temp]\n",
    "        #     plt.scatter(*(hb_x_i.T), s=2, c='tab:blue')\n",
    "\n",
    "        #     # plot boundary edges\n",
    "        #     x, y = hb_x_i.T[:, hb_edges]\n",
    "        #     x, y = x.T, y.T\n",
    "        #     plt.plot(x, y, c='tab:blue', alpha=1, zorder=-1)\n",
    "\n",
    "        #     plt.gca().set_aspect('equal')\n",
    "        #     plt.grid()\n",
    "        #     plt.savefig(os.path.join(final_path, geom + f'_error_traj{np.where(error2)[0][0]}.png'))\n",
    "        #     plt.close()\n",
    "\n",
    "    for key in data['simulations']['time_steps']:\n",
    "        data['simulations']['time_steps'][key] = data['simulations']['time_steps'][key][inds_to_keep]\n",
    "\n",
    "    data['simulations']['trajectories']['error_flag'] = error2.astype(bool)\n",
    "    data['simulations']['trajectories']['ends_in_contact'] = ends_in_contact.astype(bool)\n",
    "\n",
    "    # sum up is_bifurcation_point per trajectory\n",
    "    inds_slice = np.nonzero(np.diff(data['simulations']['time_steps']['trajectory']))[0] + 1\n",
    "    inds_slice = np.insert(inds_slice, 0, 0)  # add zero at the beginning\n",
    "    data['simulations']['trajectories']['contains_bifurcation'] = np.add.reduceat(data['simulations']['time_steps']['is_bifurcation_point'], inds_slice).astype(bool)\n",
    "\n",
    "    # ====================== SAVE DATA ======================\n",
    "    with open(os.path.join(final_path, geom + '.pkl'), 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "path = r'E:\\OneDrive - TU Eindhoven\\Werk\\python\\wallpaper_dataset\\data\\dataset1\\errors.pkl'\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(errors, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = r'E:\\OneDrive - TU Eindhoven\\Werk\\python\\wallpaper_dataset\\data\\dataset1\\errors.pkl'\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(errors, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty print all data: keys, shapes, dtype\n",
    "for key, value in data.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(key)\n",
    "        for key2, value2 in value.items():\n",
    "            if isinstance(value2, dict):\n",
    "                print(f'    {key2}')\n",
    "                for key3, value3 in value2.items():\n",
    "                    if isinstance(value3, dict):\n",
    "                        print(f'        {key3}')\n",
    "                        for key4, value4 in value3.items():\n",
    "                            print(f'            {key4}: {value4.shape} {value4.dtype}')\n",
    "                    elif isinstance(value3, np.ndarray):\n",
    "                        print(f'        {key3}: {value3.shape} {value3.dtype}')\n",
    "                    else:\n",
    "                        print(f'        {key3}: {value3}')\n",
    "            elif isinstance(value2, np.ndarray):\n",
    "                print(f'    {key2}: {value2.shape} {value2.dtype}')\n",
    "            else:\n",
    "                print(f'    {key2}: {value2}')\n",
    "    elif isinstance(value, np.ndarray):\n",
    "        print(f'{key}: {value.shape} {value.dtype}')\n",
    "    else:\n",
    "        print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create .png for each geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mesh_funcs as mf\n",
    "\n",
    "save_dir = r'data\\dataset1\\final_dataset\\images'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "path = r'data\\dataset1\\final_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set font size bigger\n",
    "plt.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.pkl'):\n",
    "        with open(os.path.join(path, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        group = data['geometry']['fundamental_domain']['group']\n",
    "        shape = data['geometry']['fundamental_domain']['shape']\n",
    "        uc_bounds = data['geometry']['unit_cell']['bounds']\n",
    "        fd_bounds = data['geometry']['fundamental_domain']['bounds']\n",
    "\n",
    "        # Plot unit cell 2Ã—2\n",
    "        fig, ax = plt.subplots(figsize=(14, 14))\n",
    "        ax.set_title(f'{group} ({shape})')\n",
    "        fig.patch.set_facecolor(\"None\")\n",
    "\n",
    "        # plot black boundary around unit cell\n",
    "        temp = np.stack(uc_bounds)\n",
    "        temp[:, 1] += temp[:, 0]\n",
    "        ax.plot(*temp.T, c='black')\n",
    "        # plot black boundary around fundamental domain\n",
    "        temp = np.stack(fd_bounds)\n",
    "        temp[:, 1] += temp[:, 0]\n",
    "        ax.plot(*temp.T, c='black')\n",
    "\n",
    "        uc_corners = data['geometry']['unit_cell']['corners']\n",
    "        fd_corners = data['geometry']['fundamental_domain']['corners']\n",
    "        # background fundamental domain shape\n",
    "        ax.fill(*uc_corners.T, c='lightgray', alpha=0.5, zorder=-10, antialiased=True, snap=False)\n",
    "\n",
    "        # background fundamental domain shape\n",
    "        ax.fill(*fd_corners.T, c='lightgray', alpha=0.5, zorder=-10, antialiased=True, snap=False)\n",
    "\n",
    "        all_mesh_points = data['mesh']['unit_cell']['p']\n",
    "        vecs = data['geometry']['unit_cell']['lattice_vectors']\n",
    "        all_elements = data['mesh']['unit_cell']['t']\n",
    "\n",
    "        # loop over vecs[0] translations\n",
    "        for i in range(2):\n",
    "            # loop over vecs[1] translations\n",
    "            for j in range(2):\n",
    "                points_temp = mf.translate_points(all_mesh_points, i*vecs[0]+j*vecs[1])\n",
    "\n",
    "                # plot filled triangles\n",
    "                temp = points_temp[all_elements]\n",
    "                temp = np.transpose(temp, axes=[0,2,1])\n",
    "                temp = temp.reshape(-1, temp.shape[-1])\n",
    "                temp = temp[..., [0,3,1,4,2,5]]\n",
    "                if i == 0 and j == 0:\n",
    "                    ax.fill(*temp,\n",
    "                            # antialiased=True,\n",
    "                            # snap=False,\n",
    "                            linewidth=0)\n",
    "                else:\n",
    "                    ax.fill(*temp, c='tab:orange', #antialiased=True,\n",
    "                            # snap=False,\n",
    "                            linewidth=0, edgecolor='tab:orange')\n",
    "\n",
    "                # ax.scatter(*points_temp.T, alpha=0.5, s=50)  #, c='tab:orange')\n",
    "                # x, y = np.transpose(points_temp[uc[\"edges\"].T], axes=[2,0,1])\n",
    "                # edges0 = ax.plot(x, y, alpha=0.3, c='tab:red')\n",
    "\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "        # # Rasterize the patches\n",
    "        # for artist in ax.get_children():\n",
    "        #     if isinstance(artist, patches.Patch):\n",
    "        #         artist.set_rasterized(True)\n",
    "        # fig.set_rasterized(True)\n",
    "\n",
    "        fig.savefig(os.path.join(save_dir, file[:-4] + '.png'),bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ML3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
